#+TITLE:     Class Notes
#+AUTHOR:    Dustin Leatherman

* Review & Introduction (2020/03/31)
** Review
*Orthogonal*: Vectors are orthogonal when the dot product = 0.
*** Basis

\begin{equation}
\begin{split}
\underset{(n \times 1)}{\vec{y}} = & \underset{(n \times p)}{A} \underset{(p \times 1)}{\vec{x}}\\
= & B \vec{c} \\
= & \Sigma c_i \vec{b_i} \ \text{(most $c_i$ = 0)}
\end{split}
\end{equation}

*A*: Basis Matrix

*Properties of a Good Basis*
- not all are orthogonal
- Allows for a sparse vector to be used ad the constant vector $\vec{c}$

Identity Matrices are the /worst/ basis because most coefficients are non-zero.

*2-Sparse Vector*
\begin{equation}
\begin{split}
\vec{c} = \begin{bmatrix}
0\\
0\\
0\\
0\\
3\\
0\\
0\\
4
\end{bmatrix}
\end{split}
\end{equation}


Very important!
#+begin_quote
When dealing with Natural images and a good basis, there is a sparse vector.
#+end_quote

*** Kernel
The kernel of a linear mapping is the set of
vectors mapped to the 0 vector. The kernel is often referred to as the *null
space*. Vectors should be linearly independent.

\begin{equation}
\begin{split}
Ker(A) = { \vec{x} \in \mathbb{R}^n \colon A \vec{x} = \vec{0}}
\end{split}
\end{equation}

A must be designed such that the Kernel of A does not contain any s-sparse
vector other than $\vec 0$

*Main Idea*: For (1), reduce $\vec{y}$ to a K-Sparse matrix to reduce the amount
of non-zero numbers.

** Linear Algebra Review
\begin{equation}
\begin{split}
\vec{u} = \begin{bmatrix}
1\\
2\\
-1
\end{bmatrix},
\vec{v} = \begin{bmatrix}
1\\
1\\
2
\end{bmatrix}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\underset{(1 \times 3)(3 \times 1)}{\vec{u}^T \vec{v}} = & \begin{bmatrix}
1 & 2 & -1
\end{bmatrix}\begin{bmatrix}
1\\
1\\
2
\end{bmatrix} = 1 + 2 - 2 = 1\\
= & \vec{u} \cdot \vec{v}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\underset{(3 \times 1)(1 \times 3)}{\vec{u} \ \vec{v}^T} = \begin{bmatrix}
1\\
2\\
-1
\end{bmatrix}\begin{bmatrix}
1 & 1 & 2
\end{bmatrix} = \begin{bmatrix}
1 & 1 & 2\\
2 & 2 & 4\\
-1 & -1 & -2
\end{bmatrix}
\end{split}
\end{equation}

$\vec{u} \ \vec{v}^T \neq \vec{u}^T \ \vec{v}$

*** Inner Product

\begin{equation}
\begin{split}
<\vec{a}, \vec{b}> = & \vec{a} \cdot \vec{b}\\
= & \vec{a}^T \vec{b}
\end{split}
\end{equation}

*** Cauchy-Schwartz Inequality

\begin{equation}
\begin{split}
\vec{a} = \begin{bmatrix}
1\\
2\\
-1
\end{bmatrix}, \begin{bmatrix}
1\\
1\\
2
\end{bmatrix}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
& |<\vec{a}, \vec{b}>| \leq \sqrt{1^2 + 2^2 + (-1)^2} \times \sqrt{1^2 + 1^2 + 2^2} \\
& |<\vec{a}, \vec{b}>| \leq ||\vec{a}||_2 \ ||\vec{b}||_2 \ \text{(euclidean/l2-norm)}
\end{split}
\end{equation}

*** Norms

Why is the l1 norm preferred for ML opposed to the classic l2 norm?

Philosophically,

If we looked at a sphere in l2 norm, the shadow casted would be a circle
regardless of the direction of the light.

Looking at a sphere in the l1 norm is shaped as a tetrahedron. The shadow cast
by a tetrahedron is different for different angles so observing the shadow
provides a lot more context about the sphere.

**** Euclidean/l2

*Sphere*: $||\vec{x}||_2 = \sqrt{(-4)^2 + 3^2} = \sqrt{25} = 5$

***** FOIL
Given 2 fixed vectors x,y. Consider the l2-norm squared:

$$
f(t) = ||x + ty||_2^2
$$


\begin{equation}
\begin{split}
f(t) = & ||x + ty||_2^2\\
= & <x + ty, x+ ty>\\
= & <x,x> + t <x, y> + t <y, x> + t^2 <y, y>\\
= & ||x||_2^2 + 2t<x,y> + t^2 ||y||_2^2
\end{split}
\end{equation}

#+begin_quote
Note: t<x,y> and t<y,x> can be combined because their dot-products are
equivalent. $\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}$
#+end_quote

#+begin_quote
When using Machine Learning, don't use l2 norms. Use l1
#+end_quote

***** Derivative

\begin{equation}
\begin{split}
\frac{d}{dt}(||x + ty||_2^2) = & 2<x, y> + 2t ||y||_2^2\\
= & 2 x^T y + 2t y^T y
\end{split}
\end{equation}

**** Simplex/l1

*Sphere*: $||\vec{x}||_1 = |-4| + |3| = 7$

**** Infinity

*Sphere*: $||\vec{x}||_\infty = Max{|-4|, |3|} = 4$
** Optimization

Why is Machine Learning Possible? Is there a theoretical guarantee?

#+ATTR_LaTeX: scale=0.5
[[./resources/convex2.jpg]]

Imagine A is the set of all dogs and B is the set of all Cats

If the sets are convex and do not overlap, there exists a line between them
which acts as a divider for determining whether a new pic belongs in A or B.

** Convex Set

A set is convex if whenever X and Y are in the set, then for $0 \leq t \leq 1$
the points $(1 - t)x + ty$ must also be in the set.

- #+ATTR_LaTeX: scale=0.5
[[./resources/convex1.jpg]]

** Separating Hyper-plane Theorem

Let C and D be 2 convex sets that do not intersect. i.e. the sets are
*disjoint*.

Then there _exists_ a vector $\vec{a} \neq 0$ and a number _b_ such that.

$$
a^Tx \leq b \forall x \in C
$$

and

$$
a^T x \geq b \forall x \in D
$$

The Separating Hyper-plane is defined as ${x \colon a^T x = b}$ for sets C, D.

*This is the theoretical guarantee for ML*


#+begin_quote
vector a is perpendicular to the plane b.
#+end_quote
* Why Separating Hyperplane Theorem & Subspace Segmentation Example (2020/04/07)

** Why is Separating Hyper-plane Theorem true?
*** Math Background

Let $x = d - c, \  y = u - d$
**** Square of the $l_2$-norm is the inner product
$$
\| x \|_2^2 = \langle x, x \rangle = x^T x
$$


$$
(d - c)^T (d - c) = \| d - c \|_2^2
$$
**** Expansion of Vectors

\begin{equation}
\begin{split}
& \| x + ty \|_2^2\\
= & \langle x + ty, x + ty \rangle\\
= & \| x\|_2^2 + 2t \langle x, y \rangle + t^2 \| y \|_2^2
\end{split}
\end{equation}
**** Derivative of vector products

$$
\frac{d}{dt}(\| x + ty \|_2^2) = 2 x^T y + 2t  y^T y
$$

$$
\frac{d}{dt}(\| x + ty \|_2^2)|_{t = 0 } = 2 x^T y
$$

$$
\frac{d}{dt} (\| d + t(u - d) - c \|_2^2) |_{t = 0} = 2 (d - c)^T (u - d)
$$

*** Separating Hyper-plane Theorem

C, D are convex disjoint sets. Thus there exists a vecto $\vec a \neq 0$ and a
number $b$ such that

$$
a^T x \leq b, \forall x \in C
$$

and

$$
a^T x \geq b, \forall x \in D
$$

${x: a^T x = b}$ is the separating hyper-plane for C,D.


When $b = 0$, then inconclusive answer.

*** Why is it true?


[[./resources/convex3.jpg]]

\begin{equation}
\begin{split}
\vec a^T \vec{x} \leq b \ \text{on side C}\\
\vec{a^T} \vec{x} \geq \ \text{on side D}
\end{split}
\end{equation}

*Goal*: Prove $\vec a$ exists as that means a separating hyperplane exists.


$$
dist(C, D) = min{ \| \vec{u} - \vec{v} \|_2 | \vec{u} \in C, \vec{v} \in D} = \|
\vec{c} - \vec{d} \|_2
$$

where $\| \vec u - \vec v\|_2$ is the euclidean distance.


Let $\vec a = \vec d - \vec c, \ b = \frac{1}{2}(\| \vec d \|_2^2 - \| \vec c \|_2^2)$

We will show that

$$
f(\vec x) = a^T x - b
$$

has the property that

$$
f(\vec x) \leq 0, \ \forall \vec x \in C
$$

and

$$
f(\vec x) \geq 0, \ \forall \vec x \in D
$$

Note: $(\vec d - \vec c)^T \frac{1}{2}(\vec d + \vec c) = \frac{1}{2}(\| \vec d
\|_2^2 - \| \vec c \|_2^2)$

What does showing something mean?

Let us show that $F(\vec x) \geq 0, \ \forall \vec x \in D$ (Argue by
Contradiction)


Suppose $\exists \vec{u} \in D$ such that $f(\vec{x}) < 0$

$$f(\vec{u}) = (\vec{d} - \vec{c})^T [\vec{u} - \frac{1}{2} (\vec{d} +
\vec{c})]\\
= (\vec{d} - \vec{c})^T \vec{u} - \frac{1}{2}(\| \vec{d}\|_2^2 - \| \vec{c}\|_2^2)$$

*Subtract 0*

$$
f(u) = (d - c)^T [u - d + \frac{1}{2} \| d - c\|]
$$

$u - \frac{1}{2}d + \frac{1}{2} c$

$u - d + \frac{1}{2} d - \frac{1}{2} c$

$$
f(u) = (d - c)^T (u - d) + \frac{1}{2} \| d - c \|_2^2
$$

Now we observe that

$$
\frac{d}{dt}(\| d + t (u - d) - c \|_2^2) |_{t = 0} = 2 (d - c)^T (u - d) < 0
$$

and so for some small $t > 0$,

$$
 \| d + t(u - d) - c \|_2^2 < \| d - c\|_2^2
$$

$g'(t) < 0$ means decreasing. Thus $g(t) < g(0)$.

Let's call point $p = d + t (u - d)$

Then

$$
\| p - c\|_2^2 < \| d - c\|_2^2
$$

This is a contradiction. Both $d$ and $u$ are in set D. Thus by the definition
of convexity, $p = (1 - t) d + tu$

D is a convex set so p must also be in D. This situation is impossible since d
is the point in D that is closest to c.

*** Example

Let $f(\vec x) = a^T x - b$

[[./resources/convex4.jpg]]

** Subspace Segmentation Example

Machine Learning is learning the Basis A. If we can deduce that a vector $\vec
x$ is a linear combination of A, then a vector is a subspace of Basis A and we
know that it belongs to A.

$$
V_1 = {(x, y, z) \in R^3 : z = 0}
$$
$$
V_2 = {(x, y, z) \in R^3 : x = 0, y = 0}
$$

$V_i$ is the affine variety (it is also a Ring, Module)

Apply a Veronase map with degree 2 to lift up from 3 to 6 dimensions.

$\nu_n \begin{bmatrix} x\\ y\\ z \end{bmatrix} = \begin{bmatrix} x^2\\ y^2\\ z^2\\ xy\\ xz\\ yz \end{bmatrix}, \nu_n: R^3 \to R^6$

\begin{equation}
\begin{split}
z_1 = (3,4,0), z_2 = (4,3,0),\\
z_3 = (2, 1, 0), z_4 = (1, 2, 0),\\
z_5 = (0, 0, 1), z_6 = (0, 0, 3), z_7 = (0, 0, 4)
\end{split}
\end{equation}

Plug the sample points into the Veronase map to produce a matrix L

$$
L = \begin{bmatrix}
9 & 16 & 4 & 1 & 0 & 0 & 0\\
16 & 9 & 1 & 4 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 9 & 6\\
12 & 12 & 2 & 2 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
\end{bmatrix} \in R^{6 \times 7}
$$

solve for $\vec c$, where $\vec c^T L = \vec 0$

$\vec c_1 = \begin{bmatrix} 0\\ 0\\ 0\\ 0\\ 1\\ 0 \end{bmatrix}, \vec c_2 = \begin{bmatrix} 0\\ 0\\ 0\\ 0\\ 0\\ 1 \end{bmatrix}$

Rank(L) = 4 (since there are 4 linearly independent rows)

\begin{equation}
\begin{split}
q_1(X) = & \vec c^T \nu_n (X)\\
= & xz\\
q_2(X) = & \vec c_2^T \nu_n (X)\\
= & yz
\end{split}
\end{equation}

We have:

\begin{equation}
\begin{split}
q_1(X) = xz & \ \ \ V_1 = (z = 0)\\
q_2(X) = yz & \ \ \ V_2 = (x = 0, y = 0)
\end{split}
\end{equation}

Observe:

$V_1 \cup V_2 = ((x,y,z) \in R^3: q_1(X) = 0, q_2(X) = 0)$

Construct the Jacobian matrix

J(Q)(X) = $\begin{bmatrix} \frac{\partial q_1}{\partial x} & \frac{\partial q_1}{\partial y} & \frac{\partial q_1}{\partial z}\\ \frac{\partial q_2}{\partial x} & \frac{\partial q_2}{\partial y} & \frac{\partial q_2}{\partial z}\end{bmatrix} = \begin{bmatrix} z & 0 & x\\ 0 & z & y \end{bmatrix}$

1. When $z = z_1 = (3, 4, 0), J(Q)(z_1) = \begin{bmatrix} 0 & 0 & 3\\ 0 & 0 & 4 \end{bmatrix}$

   When $z = z_3 = (2, 1, 0)$, $J(Q)(z_3) = \begin{bmatrix} 0 & 0 & 2\\ 0 & 0 & 1 \end{bmatrix}$

   The right null space of $J(Q)(z_1)$ has basis $\vec b_1 = \begin{bmatrix} 1\\ 0\\ 0 \end{bmatrix}$, $\vec b_2 = \begin{bmatrix} 0\\ 1\\ 0 \end{bmatrix}$

2. When $z = z_5 = (0,0,1)$, $J(Q)(z_5) = \begin{bmatrix} 1 & 0 & 0\\ 0 & 1 & 0
   \end{bmatrix}$

   When $z = z_7 = (0, 0, 4)$, $J(Q)(z_7) = \begin{bmatrix} 4 & 0 & 0\\ 0 & 4 & 0 \end{bmatrix}$
   The right null space of $J(Q)(z_5)$ has basis $\vec b = \begin{bmatrix} 0\\ 0\\ 1 \end{bmatrix}$

$C = [\vec c_1 | \vec c_2]$
* Sparse Representation & Problem P0 . P1 (2020/04/14)
** Big Idea

Your Data is a vector $x \in R^N$ where all vectors are column
vectors. Each x is s-sparse i.e. each vector has at *most* _s_ non-zero entries. Let s
= 5000. We don't know where the non-zero entries are located.

Let $\underset{(m \times N)}{A}, \ m < N$

$N = 100,000, \ m = 20,000$

Short + Wide Matrix

#+begin_quote
This is the opposite of the kinds of matrices seen in Linear Regression which
are tall and skinny.
#+end_quote

What if we can design a matrix $A \in R^{m \times N}$ so that for each s-sparse
$\vec x \in R^N$, you can store $\vec y$ instead? ($A \vec x = \vec y$)

Q: Is there a way to get back $\vec x$ from $\vec y$? We observe $\vec y$.

A: Yes!

*Properties of $A$*
- A cannot be the 0 matrix.
- if $\vec x_1$ is s-sparse and $\vec x \neq 0$, what if $\vec x_1$ is in
  $ker(A)$? No! that would return $\vec 0$ which means we cannot reconstruct the
  original matrix since there are multiple vectors in Ker(A).


*Using Techniques from 1955*

1. Is $\vec x$ the inverse of $\vec y$ or psuedo-inverse, or Moore-Penrose
  inverse, or...?

\begin{equation}
\begin{split}
\vec y = & A \vec x\\
A^{\#} \vec y = & A^{\#} A \vec x \ \text{where} \ $A^{\#} A = I$
\end{split}
\end{equation}

Doesn't work! This is because there is no way to guarantee that $\vec x$ is a
s-sparse vector.

2. Can we use gradient descent to solve for $\vec x$ to minimize $\| \vec y - A \vec x \|_2$

   No! Why?

   pick any vector $\vec v \in Ker(A)$. $\vec y = A (\vec x + \vec v)$ however,
   $(\vec x + \vec v)$ may not be sparse.


New math was needed to solve this problem so it was created in 2005 by Donoho,
Candes, and Tao using the $l_1$-norm instead of the euclidean norm ($l_2$).

** Background

*$l_1$-norm*: $\|x\|_1 = |x_1| + |x_2| + |x_3|$

*$l_2$-norm*: $\|x\| = \sqrt{|x_1|^2 + |x_2|^2 + |x_3|^2}$

For $\vec x \in R^n, \ \vec y \in R^N$, then

$$
\| \vec x + \vec y \| \leq \| x \|_1 + \| y \|_1
$$

For a norm to be valid, it must uphold the *Triangle Inequality*.

$\vec a$ is one side of a triangle, $\vec b$ is a second side, third side, ...

\begin{equation}
\begin{split}
|\vec a + \vec b | \leq |\vec{a}| + |\vec{b}|\\
\| \vec{x} + \vec{y} \|_1 \leq \|\vec{x}\|_1 + \|\vec{y}\|_1\\
\| \vec{x} + \vec{y} \|_2 \leq \|\vec{x}\|_2 + \|\vec{y}\|_2\\
\| \vec{x} + \vec{y} \|_2 \leq \|\vec{x}\|_\infty + \|\vec{y}\|_\infty\\
\end{split}
\end{equation}


It also must be distributive:

If $\vec x_1 + \vec x_2 = \vec y$, then $(\vec x_1 + \vec x_2) \cdot \vec a =
\vec{y} \cdot \vec{a}$ for any $\vec a$

$$
\langle \vec x_1 + \vec x_2, \vec a \rangle = \langle \vec y, \vec a \rangle
\to\\
\langle \vec x_1, \vec a \rangle + \langle \vec x_2, \vec a \rangle = \langle
\vec y, \vec a \rangle
$$

** Warm-up

$A = [\vec a_1 | ... | \vec a_N]$

$\| \vec a_j \|_2 = 1 = \langle \vec a_j, \vec a_j \rangle$


Let $\vec v \in Ker(A), \ \vec{v} \neq \vec 0, \ \vec v = \begin{bmatrix} v_1 \\ v_2 \\ ... \\ v_N \end{bmatrix}$

Assume $\vec a_j$ are unit vectors.

Pick $i = 3$ observations.

1. Multiply by 1. Be Sneaky.

   $v_i = v_i \langle \vec a_i, \vec a_i \rangle$

2. $\vec v \in Ker(A)$

\begin{equation}
\begin{split}
& v_1 a_1 + v_2 a_2 + ... + v_n a_n = \vec 0\\
\to & \langle v_1 a_1 + ... + v_N a_N, a_i \rangle = \langle \vec 0, a_i \rangle\\
\to & \langle v_1 a_1, a_i \rangle + ... + \langle v_N a_N, a_i \rangle = \langle \vec 0, a_i \rangle
\end{split}
\end{equation}

Keep $v_3 \langle a_3, a_i \rangle$ on the left side. Move everything to the
other side. Thus,

$$
v_i = \langle v_i a_i, a_i \rangle = - \sum_{j = 1, j \neq i}^{} v_j \langle a_j, a_i \rangle
$$

Since $i = 3$, $v_3 \langle a_3, a_i \rangle = v_i$

$$| v_i | \leq \sum_{j = 1, j \new i} | v_j | \cdot | \langle a_j, a_i \rangle |$$

What is the absolute value of a single number in $Ker(A)$? There is a relation
between $v_i$ and the rest of the entries in $\vec v$.

#+begin_quote
Why "=" becomes $\leq$

For example,
if -2 = 3 + (- 5), then
|-2| leq |3| + |-5| 
#+end_quote

** Getting Ready to Formulate the Problem

*** Problem P0

Find the s-sparse $\vec x \in R^N$ such that $\vec y = A \vec x$.

Ex. Problem 1 HW 1.

Find a 2-sparse vector $\vec x \in R^8$ such that $\vec y = A \vec x$.

There are $8 \choose{2}$ 2-sparse vectors. (28).

Imagine N = 100,000 and s = 5000. Not feasible to try all sparse-vectors.

*** Problem P1 (Convex Optimization)

Given $A \in R^{m \times N}$ and measurement $\vec y = R^m$, solve the
optimization problem,

$$
\underset{x \in R^N}{min} \| x\|_1
$$

subject to constraint $y = A \vec x$

Find a condition on matrix A, so that solving P1 will recover the s-sparse
vector $x \in R^N$

** Null Space Property of Order s
*** Setting up Notation
Let $\vec v \in Ker(A), \ \vec v \neq \vec 0$

Let the set of indices , where $\vec v [j] \neq 0$ to be S.

e.g. $\vec x = \begin{bmatrix} 0\\ 0\\ 2\\ 2\\ 3\\ 0\\ 4 \end{bmatrix}$

$S = \{3, 5, 7\}$ (non-zero indices. Also called the support vector of $\vec v$).

$|S| = s$ (number of elements. i.e. sparsity)

$\bar S = \{1, 2, 4, 6\}$ (complement. i.e zero indices)


$$
\vec v = \begin{bmatrix}1\\ 1\\ 1\\ 1\\ 2\\ -2\\ 2\end{bmatrix}, \vec v_S
= \begin{bmatrix} 0\\ 0\\ 1\\ 0\\ 2\\ 0\\ 2 \end{bmatrix}, \ \vec v_{\bar S}
= \begin{bmatrix} 1\\ 1\\ 0\\ 1\\ 0\\ -2\\ 0 \end{bmatrix}
$$

$\vec v = \vec v_S + \vec v_{\bar S}$

*** Definition

Let A be a $m \times N$ matrix.

Let S be a subset or $\{1,2,3,...,N\}$. Suppose $N = 50$, and $S = \{3,5,7\}$

1. We say that a matrix A satisfies the null space property with respect to a
   set S if
   $$
   \| \vec v_S \|_1 < \| \vec_{\bar S} \|, | \forall \vec v \in Ker(A)
   $$
2. If it satisfies the null space property with respect to any set S of size s
   where S is a subset of $\{1,2,3,...,N\}$. $s < N$

If a matrix satisfies this property, what does it buy us?

If a matrix A satisfies the Null Space property of order s, then solving problem
P1 will solve P0. i.e. you can recover any s-sparse vector $\vec x$ from the
measurement $y$ where $\vec y = A \vec x$

#+begin_quote
If A has a small coherence, then it satisfies the Null Space Property of order s.
#+end_quote

Let $A = [\vec a_1| ... | \vec a_N]$

$$
\mu_1 = \underset{j \neq k}{max} |\langle \vec a_j, \vec a_k \rangle|
$$

Assume $\vec a_j$ has $l_2$-norm equal to 1.

*** Theorem

Same assumptions as above.

Suppose $\mu_1 \cdot s + \mu_1 \cdot (s - 1) < 1$

The matrix satisfies the Null Space property of order s.

*Remarks*
1. $\mu_1 (2s - 1) < 1$ if true, then A satisfies NSP of order s. It is not a
   necessary condition. It is a sufficient condition.
2. From the warm up, if we fix an index i, then for $\vec v \in Ker(A)$,
\begin{equation}
\begin{split}
|v_i| \leq \sum_{j = 1, j \neq i}^{} |v_j| \cdot |\langle \vec a_j, \vec a_i \rangle|
\end{split}
\end{equation}
3. Note that $|v_i|$ is just one term in $\|v\|_1$ because

   $$
   \|v\|_1 = |v_1| + |v_2| + ...
   $$

*** Proof

Given A is an $m \times N$ matrix. $A = [\vec a_1 | ... | \vec a_N]$.

Suppose $\|\vec a_j\| = 1, \ \mu_1 \cdot s + \mu_1 \cdot (s - 1) < 1$

Show that NSP of order s holds.

i.e.
$$
\|\vec v_S \| < \| \vec v_{\bar S}\|, \forall \vec v \in ker(A)| \{\vec 0\}
$$

and for every set

$$
S \subset \{1,2,3,...,N\} \text{with} |S| = s
$$

Let $\vec v = Ker(A)$

$\vec v = \begin{bmatrix} v_1\\ v_2\\ ...\\ v_n \end{bmatrix}$

$A \vec v = v_1 \vec a_1 + ... + v_N \vec a_N = \vec 0$

Let $S \subset \{1,2,...,N\}, \ |S| = s$. Pick any $\vec a_i, i \in S$

Then $v_i = v_i \langle \vec a_i, \vec a_i \rangle$. Also, $v_1 \langle \vec a_i, \vec a_i \rangle + ... + v_N \langle \vec a_N, \vec a_i \rangle = 0$

\begin{equation}
\begin{split}
\to v_i = v_i \langle \vec a_i, \vec a_i \rangle = - \sum_{j = 1, j \neq i}^{}  v_i \langle \vec a_j, \vec a_i \rangle\\
\to v_i = - \sum_{l \in S}^{} v_l \langle \vec a_l, \vec a_i \rangle - \sum_{j \in S, j \neq i}^{} v_j \langle \vec a_j, \vec a_i \rangle\\
\to |v_i| \leq \sum_{l \in S}^{} |v_l| |\langle \vec a_l, \vec a_i \rangle| + \sum_{j \in S, j \neq i}^{} |v_j| |\langle \vec a_j, \vec a_i \rangle|
\end{split}
\end{equation}

sum over all $i \in S$ to get

$\|\vec v_S\|_1 = \sum_{i \in S}^{} |v_i|$

#+begin_quote
This adds up all the inequalities for one inequality to rule them all.
#+end_quote

\begin{equation}
\begin{split}
\leq & \sum_{i \in S}^{} \sum_{l \in \bar S}^{} |v_l| \cdot |\langle \vec a_l, \vec a_i \rangle| + \sum_{i \in S}^{} \sum_{j \in S, j \neq i}^{} |v_j| \cdot |\langle \vec a_j, \vec a_i \rangle| \\
= & \sum_{l \in \bar S}^{} |v_l| \sum_{i \in S}^{} |\langle \vec a_l, \vec a_i \rangle| + \sum_{j \in S}^{} |v_j| \sum_{i \in S, i \neq j}^{} |\langle \vec a_j, \vec a_i \rangle|\\
\leq & \sum_{l \in S}^{} |v_l| \mu_1 \cdot s + \sum_{j \in S}^{} |v_j| \mu_1 (s - 1)\\
\|\vec v_S\|_1 \leq & \mu_1 \cdot s \|\vec v_{\bar S}\| + \mu_1 (s - 1) \|\vec v_\S\|
\end{split}
\end{equation}

$$
(1 - \mu_1 (s - 1)) \|\vec v_{\bar S}|\ \leq \mu_1 \cdot s \|\vec v_S\|
$$


Since $\mu_1 (s - 1) + \mu_1 (s) < 1$ by hypothesis, so $1 - \mu_1 (s - 1) \geq
\mu_1 (s)$  and hence $\|\vec v_S\|_1 < \|\vec v_{\bar S}\|_1$

** Ways to Solve P1

There are 8 algos to solve P1. The worst performing one is Linear programming.

This is one of the Algos

*** Algos

$A = \begin{bmatrix}1 & 1\end{bmatrix}$
$\vec x = \begin{bmatrix} x_1\\ x_2 \end{bmatrix}$

$a_{11} = a_{12} = 1$

$Q = \begin{bmatrix} \frac{1}{w_1} & 1\\ 0 & \frac{1}{w_2}\end{bmatrix}$
1. Minimize $\|\vec x_1\|$ subject to $\vec y = A \vec x$

\begin{equation}
\begin{split}
\vec y = & (A A^T) (A A^T)^{-1} \vec y\\
\vec y = & A (A^T (A A^T)^{-1} \vec y)
\end{split}
\end{equation}

Why not let $\vec x = (A^T (A A^T)^{-1} \vec y)$

maybe we can do better.

$\vec y = A Q A^T (A Q A^T) \vec y$

Why not let $\vec x = (Q A^T (A Q A^T)^{-1} \vec y)$

How to choose Q?

2. $min \sum_{i = 1}^{N} W_i x_i^2$ subject to $\vec y = A \vec x$

   This is not the $l_1$-norm but it would be if $w_i = \frac{1}{|x_i|}$.

   solve 2. then substitute $w_i$

3. min: $w_1 x_1^2 + w_2 + x_2^2$ subject to $y = a_{11} x_1 + a_{12} x_2$

   $f(x_1) = w_1 x_1^2 + w_2 (y - x_1)^2$

   $f'(x_1) = 0$ solve for x_1

   $2 w_1 x_1 + 2 (y - x_1)(-1)w_2 = 0$

   $x_1 = \frac{w_2}{w_1 + w_2} y, \ x_2 = \frac{w_1}{w_1 + w_2}v$


\begin{equation}
\begin{split}
AQA^T = & \begin{bmatrix}1 & 1 \end{bmatrix} \begin{bmatrix} \frac{1}{w_1} & 0 \\ 0 & \frac{1}{w_2} \end{bmatrix} \begin{bmatrix}1\\ 1 \end{bmatrix}\\
= & \frac{w_1 + w_2}{w_1 w_2}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
QA^T(AQA^T)^{-1} y = \begin{bmatrix} \frac{1}{w_1}\\ \frac{1}{w_2} \end{bmatrix} \frac{w_1 w_2}{w_1 + w_2} y
\end{split}
\end{equation}
