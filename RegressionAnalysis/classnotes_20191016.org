* 2019/10/16 Classnotes

** Sum of Squares

\begin{equation}
\begin{split}
\underset{(1 \times n)(n \ times 1)}{\vec{Y}^T \vec{Y}} = \Sigma Y_i^2
\end{split}
\end{equation}

*Quadratic Form*: Contains squares of observations *and* their cross products.
 These are known as second-degree polynomials.

 Quadratic forms scaled by $\sigma^2$ allow us to treat the random variable Y as
 an observation of $\chi^2_{n - 1}$ distribution.

 This is unlike $\sigma^2 (A \vec{Y}) = A \sigma^2 (\vec{Y}) A^T$ since that is
 squaring a matrix of *constants* whereas $\vec{Y}^T \vec{Y}$ squares a matrix
 of *random variables* i.e. Y

*** SSE
\begin{equation}
  \begin{split}
   SSE = & \Sigma e_i^2\\
       = & \vec{e}^T \vec{e}\\
       = & \vec{Y}^T (I - H) \vec{Y}
  \end{split}
\end{equation}

*** SSTo

\begin{equation}
  \begin{split}
    SSTo = & \Sigma (Y_i - \bar{Y})^2\\
         = & \Sigma Y_i^2 - \frac{(\Sigma Y_i)^2}{n}\\
         = & \vec{Y}^T (I - \frac{1}{n} J) \vec{Y}
  \end{split}
\end{equation}

*** SSR

\begin{equation}
  \begin{split}
    SSR = & \Sigma (\hat{Y_i} - \bar{Y})^2\\
        = & \vec{Y}^T (H - \frac{1}{n} J) \vec{Y}
  \end{split}
\end{equation}

** Mean Estimates $\sigma^2$
*** Mean Responses
$\hat{Y_h} = b_0 + b_1 X_h$

so? we would like $\underset{(1 \times 1)}{\hat{Y_h}} =
\begin{bmatrix}
1 & X_h 
\end{bmatrix}
\vec{b}$

Let $\vec{X_h} = \begin{bmatrix}
1\\
X_h
\end{bmatrix}$

Then, $\hat{Y_h} = \vec{X_h}^T \vec{b}$

This is an estimate of the mean response!
** Variance of $\hat{Y_h}$
\begin{equation}
  \begin{split}
    \underset{(1 \times 1)}{Var(\hat{Y_h})} = & Var(\vec{X_h}^T \vec{b})\\
                                            = & \vec{X_h}^T Var(\vec{b}) \vec{X_h}\\
                                            = & \vec{X_h}^T \sigma^2(X^T X)^{-1} \vec{X_h}\\
                                            = & \underset{(1 \times 2)(2 \times 2)(2 \times 1)}{\sigma^2 X_h^T (X^T X)^{-1} \vec{X_h}}
  \end{split}
\end{equation}
** Multiple Regression Models

$Y_i = \beta_0 + \beta_1 X_{i1} + ... + \beta_{p - 1} X_{i, p - 1} + \epsilon_i$
where $\epsilon_i \sim iid N(0, \sigma^2)$

$E(Y_i) = \beta_0 + \beta_1 X_{i1} + ... + \beta_{p - 1} X_{i, p - 1}$

$Y_i \sim indep N(E(Y_i), \sigma^2)$.

The parameters of this model are {\beta_0, ..., \beta_p}. Thus there are *p*
regression coefficients.

*** Interpretation
Using the model, $Y_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2}$

let's interpret the coefficients.

$\beta_0$: The mean response of Y when $X_1 = 0, X_2 = 0$

$\beta_1$: For a fixed value of $X_2$, the associated increase in mean response
in Y is $\beta_1$ for every 1 unit increase in $X_1$. *This is known as a
partial effect*

$\beta_2$: For a fixed value of $X_1$, the associated increase in mean response
in Y is $\beta_2$ for every 1 unit increase in $X_2$.

$\beta_k$: Associated change in mean response of Y for every 1 unit increase in
$X_k$, given all other predictors are held constant.

*** Aside: Multi-Collinearity
*Multicollinearity* occurs when two or more predictors are highly correlated.
- Standard Errors blow up which makes test statistic small, which makes p-values
  high. This affects the ability for us to make *inferences*
- Multicollinearity is acceptable when using models for *prediction* but not
  when using them for *inference*.
*** Matrix Notation
\begin{equation}
  \begin{split}
    \underset{(n \times 1)}{\vec{Y}} = & \underset{(n \times p)(p \times 1)}{X \vec{\beta}} + \underset{(n \times 1)}{\vec{\epsilon}}\\
    \underset{(n \times n)}{Var(\vec{\epsilon})} = & \sigma^2 I
  \end{split}
\end{equation}
**** Fitted Values
$\hat{Y_i} = b_0 + b_1 X_{i,1} + ... + b_{p - 1} X_{i, p - 1}$

*Residuals*: $e_i = Y_i - \hat{Y_i}$

**** Least Squares Estimators

$\underset{(p \times 1)}{\vec{b}} = \underset{(p \times n)(n \times p)}{(X^T
X)^}{-1}} \underset{(p \times n)(n \times 1)}{X^T \vec{Y}}$
*** ANOVA Table

| Source     | SS                                     | DF    | MS                      | F                       | p-value                    |
|------------+----------------------------------------+-------+-------------------------+-------------------------+----------------------------|
| Regression | SSR = $\Sigma (\hat{Y_i} - \bar{Y})^2$ | p - 1 | MSR = \frac{SSR}{p - 1} | $F^* = \frac{MSR}{MSE}$ | $P(F_{p-1, n-p} \geq F^*)$ |
| Error      | SSE = $\Sigma (Y_i - \hat{Y_i})^2$     | n - p | MSE = \frac{SSE}{n - p} |                         |                            |
| Total      | SSto = $\Sigma (Y_i - \bar{Y})^2$      | n - 1 |                         |                         |                            |

*** Omnibus F-Test for Regression Relation
\begin{equation}
  \begin{split}
    H_0: \beta_1 = \beta_2 = ... = \beta_p = 0\\
    H_A: \text{ at least one } \beta_k \neq 0
  \end{split}
\end{equation}

Test statistic: $F^* = \frac{MSR}{MSE}$.
If $H_0$ is true, $F^* \sim F_{p - 1, n - p}$
*** Coefficient of Multiple Determination
$R^2 = 1 - \frac{SSE}{SSTo}$

The issue with $R^2$ is that it increases with the number of predictors
*irrespective* of the predictor improving the model.

$R_{adj}^2 = 1 - \frac{\frac{SSE}{n - p}}{\frac{SSTo}{n - 1}}$
*** Coefficient of Multiple Correlation

$R = \sqrt{R^2}$

*** Inferences in $\beta_k$
\begin{equation}
  \begin{split}
    H_0: \beta_k = 0\\
    H_A: \beta_k \neq 0
  \end{split}
\end{equation}

*Test Statistic*: $t^* = \frac{b_k}{SE_{bk}}$

If $H_0$ is true, then $t^* \sim t_{n - p}$

p-value = $2 P(t_{n - p} \geq |t|)$

#+BEGIN_SRC R
2 * (1 - pt(abs(t.star), n - p))
#+END_SRC

$100(1 - \alpha)%$ C.I. for $\beta_k$: $b_k \pm t_{1 - \frac{\alpha}{2}, n - p} SE_{bk}$
