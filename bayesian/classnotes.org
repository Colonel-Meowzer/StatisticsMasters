#+TITLE:     Bayesian Statistics
#+AUTHOR:    Dustin Leatherman

* Intro (2020/09/10)

** Motivating Example
There are two students: Student A and Student B, along with an instructor.
A secretly writes down a number (1,..,10) then mentally calls heads or tails.

1. The instructor flips a coin
2. If heads, A honestly tells B if the number is even or odd.
3. If A guesses H/T correctly, A tells B if their number is even or odd.
   Otherwise, they lie.
4. B will guess if the number is odd or even

   Let $\theta$ be the probability that B correctly guesses even or odd.

#+begin_quote
The class (and myself) initially agreed without much discussion that 0.5 is the
obvious answer. Upon further thinking on this, the probabilities breakdown in
such way:

(2): 0.5
(3): 0.5
(4): ?

The initial logic is that its a 50/50 chance since there are two choices but
there is an X-factor here with number 4. A few questions worth asking:
1. Does B know the rules upfront? As in, are they
aware that A may or may not lie?
2. Does B see the result of the coin flip?
3. Is this done virtually or in person?

If the answer is no for 1 and 2, then 0.5 is a logical choice because they'd
be guessing without much foreknowledge.

If the answer is yes for 1 and 2, then B is in on the "game" and can make a more
educated guess. If A or the professor has a "tell", then that could provide
information. Reading body language may also provide some information to B on the
veracity of A's claim.

I would argue that $\theta$ would be > 0.5 /if/ A and B know each other well
enough. Which is really a great example of Bayesian vs Frequentist view points.
#+end_quote


** Frequentist Approach

Quantifies uncertainty in terms of repeating the process that generated the data
many times.

*** Properties
- The parameters $\theta$ are fixed, unknown, and a constant.
- The sample (data) Y are random
- All prob. statements would be made about the randomness in the data.
-

A statistic $\hat \theta = Y / n$ is a statistic and is an estimator of the
population proportion $\theta$

The distribution of $\hat \theta$ from repeated sampling is the /sample distribution/.
*** Things a Frequentist would never say
- $P(\theta > 0) = 0.6$ because $\theta$ is not a random variable
- The distribution of $\theta$ is Normal(4.2,1.2)
- The probability that the true proportion is in the interval (0.4, 0.5) is
  0.95.
- The probability that the null hypothesis is true is 0.03.


** Bayesian Approach

Expresses uncertainty about $\theta$ using probability distributions. $\theta$
is still fixed and unknown.

Distribution /before/ observing the data is the *prior distribution*. e.g.
$P(\theta > 0.5) = 0.6$. This is subjective since people may have different priors.

Hopefully, Uncertainty about $\theta$ is reduced after observing the data.

Bayesian Interpretations differ from /Frequentist/ Interpretations.

Uncertainty distribution of $\theta$ after observing the data is the *posterior
distribution*.

*Bayes Theorem* for updating the prior

\begin{equation}
\begin{split}
f(\theta | Y) = \frac{f(Y | \theta) f(\theta)}{f(Y)}
\end{split}
\end{equation}

Described in words: Posterior $\propto$ Likelihood $\times$ Prior

#+begin_quote
$f(\theta | Y)$ is the posterior distribution.

Given that I have seen some data, what am I seeing now?
#+end_quote

A key difference between Bayesian and frequentist statistics is that all
inference is conditional on the single data set we observed (Y).



*** Likelihood Function

Distribution of the observed data given the parameters. This is the Same
function used in a maximum likelihood analysis.

#+begin_quote
When prior information is weak, Bayesian and Maximum Likelihood Estimates are similar.
#+end_quote

*** Priors

Say we observed Y = 60 successes in n = 100 trials and $\theta \in [0,1]$ is the
true probability of success.

Want to select a prior that has a domain of [0, 1]

If there is no relevant prior information, we might use $\theta \sim Uni(0,1)$.
This is called an /uninformative prior/. aka a "best guess".

**** Beta

Beta distributions are a common prior for parameters between 0 and 1.

If $\theta \sim Beta(a, b)$, then the posterior is

$$
\theta | Y \sim Beta(Y + a, n - Y + b)
$$


$Beta(1,1) == Uni(0,1)$

**** Gamma
Popular distribution for $\sigma$ (population standard deviation)

*** Posteriors

The likelihood function $Y | \theta \sim Bin(n, \theta)$

The Uniform prior is $\theta \sim Uni(0, 1)$

The posterior is then $\theta | Y \sim Beta(Y + 1, n - Y + 1)$

*** Advantages
- Bayesian concepts (posterior probability of the null hypothesis) are arguably
  easier to interpret than the frequentist ideas (p-value.)
- Can incorporate scientific knowledge via the prior.
  - Even a Small amount of prior information can add stability.
- Excellent at quantifying uncertainty in complex problems.
- Provides a framework to incorporate data/information from multiple sources.

*** Disadvantages
- Less common/familiar
- Picking a prior is subjective (though there are objective priors)
- Procedures with frequentist properties are desirable.
- Computing can be slow for hard problems
- Non parametric methods are challenging

** Review

#+begin_quote
Only the interesting parts are placed here. See the rest of this repo for deeper
dives on other concepts.
#+end_quote

*** Probability
Objective (associated with Frequentist)
- $P(X = x)$ is a mathematical statement
- If we repeatedly sampled X, the value that the proportion of draws equal to x
  converges is defined as $P(X = x)$

Subjective (associated with Bayesian)
- $P(X = x)$ represents an individual's degree of belief
- Often quantified as the amount an individual would be willing to wager that X
  will be x.

A Bayesian Analysis uses both of these concepts.

*** Uncertainty

Aleatoric (def: indeterminate) uncertainty (likelihood)
- Uncontrollable randomness in the experiment

Epimestic (def: involving knowledge) uncertainty (prior/posterior)
- Uncertainty about a quantity that could be theoretically

A Bayesian Analysis uses both of these concepts

*** Probability vs Statistics

#+begin_quote
The common sense, I like the way this is phrased.
#+end_quote

Probability is the forward problems
- We assume we know how the data are being generated and computer the
  probability of events.

  For example, what is the probability of flipping 5 straight heads if the coins
  are fair?

Statistics is the inverse problem
- We use data to learn about the data-generating mechanism

  For example, if we flipped five straight heads, can we conclude the coin is
  biased?
* Probability & Introduction to Bayes (2020/09/17)


if x and y are independent, then the following is true

$$
f(x | y) = \frac{f(x, y)}{f_Y (y)} = \frac{f_X(x) f_Y (y)}{f_Y (y)} = f_X (x)
$$


Cannot use $f(x,y)$ as PMF because $\sum_{1}^{Y} f(x,y) = f(x) \neq 1$. Need to
scale by marginal probability in order to sum to 1 and thus be a proper PMF/PDF.

|              |     1 |     2 |     3 |     4 |     5 | Total [p(y)] |
|--------------+-------+-------+-------+-------+-------+--------------|
| US           | .0972 | .0903 | .0694 | .0069 | .0069 |        .2708 |
| Not US       | .3194 | .1319 | .1389 | .1181 | .0208 |        .7292 |
| Total [p(x)] | .4167 | .2222 | .2083 | .1250 | .0278 |            1 |

show that x and y are dependent

$P(x = 1) = 0.4167$

$P(y = 1) = 0.2708$

$P(x = 1) \times P(y = 1) = 0.4167 (0.2708) = 0.1128$

$P(x = 1, y = 1) = 0.0972 \neq 0.1128$ so dependent!

** Calculating the Posterior Analytically

*** Using an Arbitrary PDF

1. Find Joint Probability (f(x,y))

\begin{equation}
\begin{split}
P(x > 7, y > 40) = & \int_{7}^{10} \int_{40}^{50} \ 0.26 exp(- |x - 7| - |y - 40|) \ dx \ dy\\
    = & 0.26 \ \int_{7}^{10} \int_{40}^{50} exp(- x + 7 - y + 40) \ dx \ dy \label{eq:21a} \ (\text{Since only interested in positive values})\\
    = & 0.26 \ \int_{7}^{10} \int_{40}^{50} exp(- (x - 7) \ exp( - (y - 40)) \ dx \ dy\\
    = & 0.26 \ \int_{7}^{10} \int_{0}^{10} exp(- (x - 7) \ exp(-u) \ dx \ du\\
    = & 0.26 \ \int_{7}^{10} \int_{0}^{10} exp(- (x - 7) \ [- exp(-u)]_0^{10} \ dx \ du\\
    = & 0.26 (1 - e^{-10}) \ \int_{7}^{10} exp(- (x - 7) dx\\
    = & 0.26 (1 - e^{-10}) (1 - e^{-3}) \approx 0.247
\end{split}
\end{equation}

2. Find Marginal Probability over the Data $f_X(x)$

\begin{equation}
\begin{split}
f_X(x) = & \int_{20}^{50} 0.26 + e^{- |x - 7| - |y - 40|} dy\\
    = & 0.26 e ^{-|x - 7|} \int_{20}^{50} e^{- |y - 40|} dy\\
    = & 0.26 e ^{-|x - 7|} [\int_{20}^{40} e^{- (40 - y)} dy +  \int_{40}^{50} e^{- (y - 40)} dy]\\
    = & 0.26 e ^{-|x - 7|} [\int_{20}^{0} - e^{-u} du +  \int_{0}^{10} e^{- u} du]\\
    = & 0.26 e ^{-|x - 7|} [1 - e^{-20} + 1 - e^{-10} \approx 2 ] \\
    = & 0.52 e ^{- |x - 7|} \ \forall \ x \leq x \leq 10
\end{split}
\end{equation}

3. Calculate Conditional Probability

$$
f(y | x) = \frac{f(x, y)}{f_X (x)} = \frac{1}{2} e^{- |y - 40|}
$$

#+begin_quote
If integrating over an absolute value, break up the integral into two integrals:
the first over the negative domain of the integration, the second over the
positive domain.
#+end_quote

*** Using Normal Distribution

1. Find Marginal Probability

\begin{equation}
\begin{split}
f(x) = & \int_{- \infty}^{\infty} \frac{1}{2 \pi \sqrt{1 - \rho^2}} \ exp(- \frac{x^2 + y^2 - 2 \rho x y}{2 (1 - \rho^2)}) \ dy\\
= & \frac{1}{2 \pi \sqrt{1 - \rho^2}} e^{-x^2 / 2(1 - \rho^2)} \ \int_{- \infty}^{\infty} \frac{1}{\sqrt{2 \pi}} exp(- \frac{y^2 - 2 \rho x y}{2 (1 - \rho^2)}) \ dy \label{eq:2b1} \ \text{(Move x's out of integral. Arrange terms so it looks like a Normal Distribution.)}\\
= & \frac{1}{\sqrt{2 \pi} \sqrt{1 - \rho^2}} e^{-x^2 / 2(1 - \rho^2)} \ \int_{- \infty}^{\infty} \frac{\sqrt{1 - \rho^2}}{\sqrt{2 \pi (1 - \rho^2)}} exp(- \frac{y^2 - 2 \rho x y + \rho^2 x^2 - (\rho x)^2}{2 (1 - \rho^2)}) \ dy\\
= & \frac{1 \ \sqrt{1 - \rho^2}}{\sqrt{2 \pi} \sqrt{1 - \rho^2}} e^{-x^2 / 2(1 - \rho^2)} \ e^{\frac{\rho x^2}{2 (1 - \rho^2)}} \ \int_{- \infty}^{\infty} \frac{1}{\sqrt{2 \pi (1 - \rho^2)}} exp(- \frac{(y - \rho x)^2}{2 (1 - \rho^2)}) \ dy, \label{eq:2b2} \ \mathnormal{N(\rho x, 1 - \rho^2)}\\
= & \frac{1}{\sqrt{2 \pi}} e^{-0.5 \ \frac{x^2 - \rho^2 x^2}{1 - \rho^2}}\\
= & \frac{1}{\sqrt{2 \pi}} e^{-0.5 x^2}, \label{eq:2b3} \mathnormal{X \sim N(0, 1)}\\
\end{split}
\end{equation}

2. Assume Joint Normal PDF

3. Find Conditional probability

\begin{equation}
\begin{split}
f(y | x) = & \frac{f(x,y)}{f_X (x)}\\
    = & \frac{\frac{1}{2 \pi \sqrt{1 - \rho^2}} exp(- \frac{x^2 + y^2 - 2 \rho x y}{2 (1 - \rho^2)})}{\frac{1}{\sqrt{2 \pi}} \ exp(- \frac{x^2}{2})}\\
    = & \frac{1}{\sqrt{2 \pi} \sqrt{1 - \rho^2}} \ exp(- \frac{x^2 + y^2 - 2 \rho x y}{2 (1 - \rho^2)} + \frac{x^2}{2})\\
    = & \frac{1}{\sqrt{2 \pi} \sqrt{1 - \rho^2}} \ exp(- \frac{1}{2} [\frac{x^2 + y^2 - 2 \rho x y}{1 - \rho^2} - x^2])\\
    = & \frac{1}{\sqrt{2 \pi} \sqrt{1 - \rho^2}} \ exp(- \frac{1}{2} [\frac{x^2 + y^2 - 2 \rho x y - (1 - \rho^2) x^2}{1 - \rho^2}])\\
    = & \frac{1}{\sqrt{2 \pi} \sqrt{1 - \rho^2}} \ exp(- \frac{1}{2} [\frac{y^2 - 2 \rho x y - \rho^2 x^2}{1 - \rho^2}])\\
    = & \frac{1}{\sqrt{2 \pi} \sqrt{1 - \rho^2}} \ exp(- \frac{1}{2} [\frac{(y - \rho x)^2}{1 - \rho^2}]), \ \label{eq:2bd} y|x \sim N(\rho x, 1 - \rho^2)
\end{split}
\end{equation}

** Bayes Theorem

$$
P(\theta | y) = \frac{P(y | \theta) P (\theta)}{P(y)}
$$

How do you know you are using Bayes Rule?

Given $P(y | \theta)$, want to find $P(\theta | y)$


- Bayesians quantify uncertainty about fixed but unknown parameters by treating
them as random variables.
- This requires that we set a prior distribution $\pi(\theta)$ to summarize
  uncertainty before observing the data.
- The distribution of the observed data given the model parameters is the
  /likelihood function/, $f(Y | \theta)$
  - The likelihood function is the most important piece of a Bayesian Analysis
    because it links the data and the parameters.

** Bayesian Learning

The posterior distribution $P(\theta | Y)$ summarizes uncertainty about the
parameters given the prior and data.

Reduction in uncertainty from prior to posterior represents *Bayesian Learning*

Bayes Theorem (again):

$$
P(\theta | Y) = \frac{f(Y | \theta) \pi (\theta)}{m(Y)}
$$

$m(Y) = \int F(Y | \theta) \pi (\theta) d \theta$: marginal distribution of the
data and can usually be ignored.

** Subjectivity

Choosing Likelihood function and a prior distribution are subjective.

If readers disagree with assumptions, findings will be rejected so assumptions
must be justified theoretically and empirically.
* Summarizing a Posterior Distribution (2020/09/24)

** SIR Model
Susceptible-Infected-Recovered

At time $t$ , $S_t + I_t + R_t = N$ where N is the population.

States evolved according to the following differential equations

\begin{equation}
\begin{split}
\frac{d S_t}{d t} = & -\beta \frac{S-t I_t}{N}\\
\frac{d I_t}{d t} = & \beta \frac{S_t I_t}{N} - \Gamma I_t
\end{split}
\end{equation}

$\beta$: Controls rate of new infections

$\Gamma$: Controls recovery rate

We will use a discrete approx to these curves with hourly time steps.

So? $dt = \frac{1}{24}$

*Goal*: Fit SIR Model for given values of $\beta$ and $\Gamma$

** Summarize a univariate Posterior with Beta-Binomial

Posterior = Likelihood $\times$ Prior

Say there is a parameter $\theta$

Likelihood: $Y | \theta \sim Bin(N, \theta)$

Prior: $\theta \sim Uni(0, 1) \equiv Beta(1,1)$

Posterior: $\theta | Y \sim Beta(Y + a, N - y + b)$

#+begin_quote
Peak of the Posterior is the MLE of the Likelihood function when using an
uninformative prior.
#+end_quote

** MAP Estimator

Posterior Mode is call the max a posterioiri (MAP) estimator

\begin{equation}
\begin{split}
\hat \theta = \underset{\theta}{argmax} \ P (\theta | y) = \underset{\theta}{argmax} \ log[f(Y | \theta)] + log[\pi(\theta)]
\end{split}
\end{equation}

if prior is uniform, MAP is MLE assuming $Y | \theta \sim Bin(\theta, n)$.

** Uncertainty Measures

Posterior Std. Dev. is one measure of uncertainty
- If approx Gaussian, can use empirical rule
- Analogous but fundamentally different than std error.
  - Std err is the standard deviation of $\hat \theta$'s sampling distribution

Do not call them call them *confidence* intervals. Called *Credible* Intervals
in Bayesian Statistics.

Interval $(l, u)$ is 100($1 - \alpha$)% posterior credible interval if $P(l < \theta < u | Y) = 1 - \alpha$

Interpretation: "Given the data and the prior, the probability that $\theta$ is
between l and u is 0.95."

#+begin_quote
Confidence interval interpretation:

With 95% Confidence, $\theta$ is between l and u.

A Bayesian Posterior is a distribution for $\theta | Y$ whereas the sampling
distribution is for $\hat \theta$. While their expected values both represent
the true mean, the sampling distribution is not a distribution of $\theta$ hence why "Confidence" is used when in the interpretation. The Bayesian Posterior is a distribution of $\theta$ so the posterior can be used for the interpretation.
#+end_quote

*** Credible Sets

Not unique.

Let $q_\tau$ be the $\tau$ quantile of the posterior of the posterior such that
$P(\theta < q_\tau | Y) = \tau$. Then ($q_{00}, q_{0.95}$), ($q_{0.01},
q_{0.96}$), etc. are all valid 95% credible sets.

Equal-Tailed intervals: $(q_{\alpha/2}, q_{1 - \frac{\alpha}{2}})$

*Highest posterior density* interval searches for the smallest interval  that
contains the proper probability

** Hypothesis Tests

Conducted by computing posterior prob of each hypothesis.

$$
P(\theta < 0.5 | Y) = \int_{0}^{0.5} P(\theta | Y) d \theta
$$

analogous but different than a p-value.

*p-value*: Assuming the null hypothesis is true, the probability we got X or a
value more extreme is Y.

*Bayesian Hypothesis Test*: Given the prior and the data, the probability the null
hypothesis is true is Y.

** Monte Carlo Sampling

A useful tool for summarizing a posterior.

In MC sampling, we draw S samples from the posterior;

$$
\theta', ..., \theta^{(s)} \sim P(\theta | Y)
$$

and use these samples to approx the posterior.

*** Transformations

MC sampling facilitates studying the *transformations* of parameters.

For example, the odds corresponding to $\theta$ are $\gamma = \frac{\theta}{1 - \theta}$

\begin{equation}
\begin{split}
\gamma^{(1)} = \frac{\theta^{(1)}}{1 - \theta^{(1)}}, ..., \gamma^{(S)} = \frac{\theta^{(S)}}{1 - \theta^{(S)}}
\end{split}
\end{equation}

How to approximate the posterior mean and variance of $\gamma$?

Transform the odds and use the draws to approximate $\theta$'s posterior!

** Summarizing Multivariate Posteriors

Univariate posteriors captured by a simple plot. Not easy or impossible to do
with multivariate posteriors.

Let $\theta = (\theta_1, ..., \theta_p)$.

Ideally, we reduced to the univariate marginal posteriors. Then the same ideas
for univariate models apply

$$
P(\theta_1 | Y) = \int ... \int P(\theta_1, ..., \theta_p | Y) d \theta_2, ...,
d \theta_p
$$

#+begin_quote
Can use Monte Carlo sampling to estimate these integrals.

Need to confirm the above statement
#+end_quote

** Bayesian One Sample t-test

Likelihood: $Y_i | \mu, \sigma \sim N(\mu, \sigma^2)$ idep over $i = 1, ..., n$
Priors: $\mu \sim N(\mu_0, \sigma_0^2)$ independent of $\sigma^2 \sim InvGamma(a, b)$

Typically we are interested in marginal posterior because it accounts for uncertainty about $\sigma^2$

Marginal Posterior: $f(\mu | Y) = \int_{0}^{\infty} P(\mu, \sigma^2 | Y) d \sigma^2, \ Y = (Y_1, ..., Y_n)$

if $\sigma$ is known, the posterior of $\mu | Y$ is Gaussian and 95% Credible
Interval is $E(\mu | Y) \pm Z_{0.975} SD(\mu | Y)$

if $\sigma$ is unknown, the marginal (over $\sigma^2$) posterior of $\mu$ is $t$
with $\nu = n + 2 a$ degrees of freedom.

$$
E(\mu | Y) \pm t_{0.975} \ SD(\mu | Y)
$$

$SD(\mu | Y)$: Standard Deviation


Can summarize results best in a table with Posterior Mean, Posterior SD, and 95%
Credible Set.

** Frequentist vs Bayesian Analysis of a Normal Mean

*Frequentist*

Estimate of the $\mu$ is $\bar Y$
If $\sigma$ is known, the 95% C.I. is: $\bar Y \pm z_{0.975} \frac{\sigma}{\sqrt
n}$

If $\sigma$ is unknown, the 95% C.I. is: $\bar Y \pm t_{0.975, n - 1} \frac{s}{\sqrt{n}}$

where $t$ is the quantile of a t-distribution.

*Bayesian*

Estimate of $\mu$ is its marginal posterior mean.

Interval estimate is 95% Credible Interval.

If $\sigma$ is known, Posterior of $\mu | Y$ is Gaussian

$E(\mu | Y) \pm Z_{0.975} SD(\mu | Y)$

If $\sigma$ is unknown, the marginal (over $\sigma^2$) posterior of $\mu$ is t
with $\nu = n + 2a$ degrees of freedom.

$E(\mu | Y) \pm t_{0.975, \nu} \ SD(\mu | Y)$

** Multiple Parameters in Multivariate Posteriors

Want to compute $P(\theta_2 > \theta_1 | Y_1, Y_2)$.

Monte Carlo sampling of the posteriors a key tool!

Model is:

\begin{equation}
\begin{split}
Y_1 | \theta_1 \sim & Bin(N, \theta_1)\\
Y_2 | \theta_2 \sim & Bin(N, \theta_2)\\
\theta_1, \theta_2 \sim & Beta(1,1)
\end{split}
\end{equation}

Marginal Posteriors both independent of each other.
- $\theta_1 | Y_1, Y_2 \sim Beta(Y_1 + 1, N - Y_1 + 1)$
- $\theta_2 | Y_1, Y_2 \sim Beta(Y_2 + 1, N - Y_2 + 1)$

#+begin_R
# Data
N <- 10; Y1 <- 5; Y2 <- 8;

# Num Samples for Monte Carlo
S <- 10000

# Posterior distributions
theta1 <- rbeta(S, Y1 + 1, N - Y1 + 1)
theta2 <- rbeta(S, Y2 + 1, N - Y2 + 1)

# True posterior mean of theta_1 per Beta distribution
(Y1 + 1) / (N + 2)

# Monte Carlo Estimate of true posterior mean of theta_1
mean(theta1)

# Monte Carlo estimate of posterior prob theta_2 > theta_1
mean(theta2 > theta1)
#+end_R

** Types of Uncertainty

*Sampling*

*Parametric*: Uncertainty about my guesses of the distribution of the parameter

*** Resolving Uncertainty
**** Plugin approach

If $\hat \theta$ is an estimate, thus $Y^* \sim f(Y | \hat \theta)$

For example, Let $\hat \theta = \frac{2}{10}$. Predict $P(Y > 0) = 1 - (1 -
0.2)^{10}$.

If $\hat \theta$ has small uncertainty, this is fine. Otherwise, this
underestimated uncertainty in $Y^*$

**** Posterior Predictive Distribution (PPD)

For the sake of prediction, the parameters aren't of interest as the parameters
are vehicles by which the data inform about the predictive model.

PPD averages over their posterior uncertainty which /accounts/ for parametric uncertainty.

$$
f(Y^* | Y) = \int f(Y^* | \theta) p(\theta | Y) \ d \theta
$$

Input == data
Output == prediction distribution

#+begin_quote
Given I've observed a certain amount of data Y, what is the distribution of the
predictor values?
#+end_quote

Monte Carlo sampling approximates the PPD.

***** Example

Let $\theta^{(1)}, ..., \theta^{(S)}$ be samples from the posterior.

Let $Y^{*(s)} \sim f(Y | \theta^{(s)})$ where $Y^{*(s)}$ are samples from the
PPD for each $\theta^{(s)}$

Posterior Predictive Mean $\approx$ sample mean of the $Y^{*(s)}$

$P(Y^* > 0) \approx$ sample proportion of non-zero $Y^{*(s)}$

#+begin_R options
# Data
Y < -2; n <- 10;

# Posterior is theta | Y sim  Beta(A,B)
A <- Y + 1;
B <- N - Y + 1

# Plugin estimate of P(Y* > 0)
1-dbinom(0,10,0.2)

# Approx PPD using MC Sampling
theta <- rbeta(100000,A,B)
Ystar <- rbinom(100000,10,theta)
mean(Ystar>0)
#+end_R
* Conjugate and Objective Priors (2020/10/01)

How do we choose priors? This is the most important step of a Bayesian Analysis.

*Key Terms*
- Conjugate vs Non-conjugate
- Informative vs Uninformative
- Proper vs Improper
- Subjective vs Objective

** Conjugate
*Def*: Prior and Posterior Distribution are from the same parametric family. This is done through a pairing of the Likelihood Distribution and the Prior Distribution.

*** Beta-Binomial

Use Case: Estimating a Proportion!

- What is the probability of success for a new cancer treatment?
- What proportion of voters support a candidate?

Let $\theta \in [0, 1]$ be a proportion we are trying to estimate.

Likelihood: $Y | \theta \sim Bin(n, \theta)$

Prior: $\theta \sim Beta(a, b)$

a: Prior number of successes
b: Prior number of failures

**** Frequentist Appraoch

MLE: $\hat \theta = \frac{Y}{n}$

$\hat \theta \sim N(\theta, \frac{\theta (1 - \theta)}{n})$ for large Y and $n -Y$

Rule of Thumb for large enough $n$ for proportions: At least 10-15 failures and 10-15 successes
depending on which text book you read.

This is slightly different than the magic number 30 which is considered large
enough for the mean.


$SE(\hat \theta) = \sqrt{\frac{\hat \theta (1 - \hat \theta)}{n}}$

**** Proof


***** Short way

The short proof uses "proportional to" ($\propto$) and handwaves the constants.


Posterior:

\begin{equation}
\begin{split}
f(\theta | Y) \propto & f(Y | \theta) f(\theta) = {n \choose Y} \theta^Y (1 - \theta)^{n - Y} \cdot \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} \theta^{a - 1} (1 - \theta)^{b - 1}\\
\propto & \theta^Y (1 - \theta)^{n - Y} \theta^{a - 1} (1 - \theta)^{b - 1}\\
= & \theta^{Y + a - 1} (1 - \theta)^{n - Y + b - 1} \label{eq:1} \ \text{(Looks like a Beta PDF)}\\
\therefore & \ \theta | Y \sim Beta(Y + a, n - Y + b)
\end{split}
\end{equation}


***** Long way

\begin{equation}
\begin{split}
f(Y | \theta) = \frac{f(Y | \theta) \cdot f(\theta)}{f(Y)} =  \frac{f(Y | \theta) \cdot f(\theta)}{\int_{0}^{1} f(Y, \theta) d \theta}
\end{split}
\end{equation}

Numerator:

\begin{equation}
\begin{split}
f(Y | \theta) f(\theta) = & {n \choose Y} \theta^Y (1 - \theta)^{n - Y} \cdot \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} \theta^{a - 1} (1 - \theta)^{b - 1}\\
= & {n \choose Y} \cdot \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)}  \theta^{Y + a - 1} (1 - \theta)^{n - Y + b - 1}\\
\end{split}
\end{equation}

Denominator:

\begin{equation}
\begin{split}
f(Y) = & \int_{0}^{1} {n \choose Y} \theta^Y (1 - \theta)^{n - Y} \cdot \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} \theta^{a - 1} (1 - \theta)^{b - 1} d \theta\\
= & {n \choose Y} \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} \int_{0}^{1} \theta^{Y + a - 1} (1 - \theta)^{n - Y + b - 1} d \theta\\
= & {n \choose Y} \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} \cdot \frac{\Gamma (Y + a) \Gamma (n - Y + b)}{\Gamma (n + a + b)} \int_{0}^{1} \frac{\Gamma (n + a + b)}{\Gamma (Y + a) \Gamma (n - Y + b)} \theta^{Y + a - 1} (1 - \theta)^{n - Y + b - 1} d \theta \label{eq:2} \ \text{(Beta PDF Integrates to 1)}\\
\text{So?} & \ f(y) = {n \choose Y} \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} \cdot \frac{\Gamma (Y + a) \Gamma (n - Y + b)}{\Gamma (n + a + b)}
\end{split}
\end{equation}

Posterior:

\begin{equation}
\begin{split}
f(\theta | Y) = & \frac{{n \choose Y} \cdot \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)}  \theta^{Y + a - 1} (1 - \theta)^{n - Y + b - 1}}{{n \choose Y} \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} \cdot \frac{\Gamma (Y + a) \Gamma (n - Y + b)}{\Gamma (n + a + b)}}\\
= & \frac{\Gamma (n + a + b)}{\Gamma (Y + a) \Gamma (n - Y + b)} \theta^{Y + a - 1} (1 - \theta)^{n - Y + b - 1}\\
\theta \sim & Beta(Y + a, n - Y + b)
\end{split}
\end{equation}

**** Shrinkage


Posterior mean: $\hat \theta_B = E(\theta | Y) = \frac{Y + a}{n + a + b}$

Posterior mean is between the sample proportion ($frac{Y}{n}$) and the prior
mean: $\frac{a}{a + b}$

$\hat \theta_B = w \frac{Y}{n} + (1 - w) \frac{a}{a + b}$

where $w = \frac{n}{n + a + b}$

- When n is large, $\hat \theta_B$ is closer to $frac{Y}{n}$.
- as a and b grow, posterior mean more dependent on the prior.

*Definition*: The gravitation between the Likelihood function and the prior
data. If there is

What prior to select if research show $\theta$ is between 0.6 and 0.8? a = 7, b
= 3 because $\frac{7}{7 + 3} = 0.7$

*** Related Problem using NegBin

Estimate the number of successes ($Y$) before n failures.

$\theta$: probability of success

$\theta \sim Beta(a, b)$

$Y | \theta \sim NegBin(n, \theta)$

\begin{equation}
\begin{split}
f(\theta  | Y) \propto & {Y + n + 1 \choose Y} \theta^Y (1 - \theta)^{n} \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} \theta^{a - 1} (1 - \theta)^{b - 1}\\
\propto & \theta^Y (1 - \theta)^{n} \theta^{a - 1} (1 - \theta)^{b - 1}\\
= & \theta^{Y + a - 1} (1 - \theta)^{n + b - 1}\\
\sim & Beta(Y + a, n + b)
\end{split}
\end{equation}
